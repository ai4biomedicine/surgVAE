{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import combinations\n",
    "from numbers import Number\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import torch\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import *\n",
    "import argparse\n",
    "from transform import *\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "with open('../../../inputs_new.pickle','rb') as handle:\n",
    "    inputs = pkl.load(handle)\n",
    "print(inputs)\n",
    "cols = inputs.preops.columns\n",
    "#get rid of first and last\n",
    "cols = cols[1:-1]    \n",
    "#change column name orlogid_encoded to orlogid\n",
    "inputs.outcomes.rename(columns={'orlogid_encoded':'orlogid'}, inplace=True)\n",
    "inputs.texts.rename(columns={'orlogid_encoded':'orlogid'}, inplace=True)\n",
    "print(inputs.outcomes.columns)\n",
    "folder = ('../preops_cv/')\n",
    "\n",
    "outcomes = ['cardiac', 'AF','arrest', 'DVT_PE', 'post_aki_status', 'total_blood']\n",
    "for idx, outcome in enumerate(outcomes):\n",
    "    foldername = folder+'arrest'+'/'\n",
    "    aurocs = []\n",
    "    auprcs = []\n",
    "    for i in range(5):\n",
    "\n",
    "        X_train = pkl.load(open(foldername + 'X_train_' + str(i) + '.pickle', 'rb'))\n",
    "        X_test = pkl.load(open(foldername + 'X_test_' + str(i) + '.pickle', 'rb'))\n",
    "\n",
    "        ids_train = pkl.load(open('../preops_cv/arrest/' + 'train_ids_' + str(i) + '.pickle', 'rb'))\n",
    "        ids_test = pkl.load(open('../preops_cv/arrest/' + 'test_ids_' + str(i) + '.pickle', 'rb'))\n",
    "        train_outcome2 = inputs.outcomes[(inputs.outcomes.orlogid.isin(ids_train))].copy()\n",
    "        train_outcome2 = train_outcome2[['cardiac', 'AF', 'arrest', 'DVT_PE', 'post_aki_status', 'total_blood']].to_numpy()\n",
    "        train_outcome2 = np.where(train_outcome2> 0, 1, 0)\n",
    "        test_outcome2 = inputs.outcomes[(inputs.outcomes.orlogid.isin(ids_test))].copy()\n",
    "        test_outcome2 = test_outcome2[['cardiac', 'AF', 'arrest', 'DVT_PE', 'post_aki_status', 'total_blood']].to_numpy()\n",
    "        test_outcome2 = np.where(test_outcome2> 0, 1, 0)\n",
    "        y_train = train_outcome2[:, idx]\n",
    "        y_test = test_outcome2[:, idx]\n",
    "\n",
    "        print(outcome + ' ' + \"fold \" + str(i))\n",
    "\n",
    "        vae_model = pkl.load(open('./distangle_vae_model/model_fold' + str(i) + '.pickle', 'rb'))\n",
    "\n",
    "        X_test = np.delete(X_test, 163, axis=1)\n",
    "\n",
    "        X_test = X_test.astype(np.float32)\n",
    "        # log transform\n",
    "\n",
    "        X_test = np.log(X_test + 1)\n",
    "        vae_model = vae_model.to(\"cuda\")\n",
    "\n",
    "        #construct model using encoder of vae and classifier\n",
    "        class VAE(nn.Module):\n",
    "            def __init__(self, vae_model):\n",
    "                super(VAE, self).__init__()\n",
    "                self.feature_encoder = vae_model.feature_encoder\n",
    "                self.encoder = vae_model.encoder\n",
    "                self.classifier = vae_model.classifiers[idx]\n",
    "            def forward(self, x):\n",
    "                x = self.feature_encoder(x)\n",
    "                x = self.encoder(x)\n",
    "                x = self.classifier(x)\n",
    "                #softmax\n",
    "                x = nn.Softmax(dim=1)(x[:, :2])\n",
    "                return x\n",
    "        model = VAE(vae_model)\n",
    "        model = model.to(\"cuda\")\n",
    "        model.train()\n",
    "\n",
    "        ig = IntegratedGradients(model)\n",
    "        x_test = torch.from_numpy(X_test).to(\"cuda\")\n",
    "        # as float32\n",
    "        x_test = x_test.float()\n",
    "\n",
    "        targets = torch.tensor(y_test).to(\"cuda\")\n",
    "        #use batch size of 64\n",
    "        dataset = torch.utils.data.TensorDataset(x_test, targets)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "        #use integrated gradients\n",
    "        atts = []\n",
    "        for batch in dataloader:\n",
    "            x, y = batch\n",
    "            x.requires_grad = True\n",
    "            attributions = ig.attribute(x, target=y, n_steps=100)\n",
    "            atts.append(attributions.cpu().detach().numpy())\n",
    "            #release memory\n",
    "            del x\n",
    "            del y\n",
    "            del attributions\n",
    "\n",
    "        #release memory\n",
    "        del x_test\n",
    "        del targets\n",
    "        del dataset\n",
    "        del dataloader\n",
    "\n",
    "        atts = np.concatenate(atts, axis=0)\n",
    "\n",
    "        #save attributions\n",
    "        if not os.path.exists('./interpret'):\n",
    "            os.makedirs('./interpret')\n",
    "        if not os.path.exists('./interpret' + '/attributions_fold_'+ outcome):\n",
    "            os.makedirs('./interpret' + '/attributions_fold_'+ outcome)\n",
    "        pkl.dump(atts, open('./interpret' + '/attributions_fold_'+ outcome + '/' + str(i) + '.pickle', 'wb'))\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import combinations\n",
    "from numbers import Number\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import torch\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import *\n",
    "import argparse\n",
    "from transform import *\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "with open('../../../inputs_new.pickle','rb') as handle:\n",
    "    inputs = pkl.load(handle)\n",
    "print(inputs)\n",
    "cols = inputs.preops.columns\n",
    "#get rid of first and last\n",
    "cols = cols[1:-1]    \n",
    "#change column name orlogid_encoded to orlogid\n",
    "inputs.outcomes.rename(columns={'orlogid_encoded':'orlogid'}, inplace=True)\n",
    "inputs.texts.rename(columns={'orlogid_encoded':'orlogid'}, inplace=True)\n",
    "print(inputs.outcomes.columns)\n",
    "folder = ('../preops_cv/')\n",
    "\n",
    "outcomes = ['cardiac', 'AF','arrest', 'DVT_PE', 'post_aki_status', 'total_blood']\n",
    "for idx, outcome in enumerate(outcomes):\n",
    "    foldername = folder+'arrest'+'/'\n",
    "    atts = []\n",
    "    for i in range(5):\n",
    "        X_train = pkl.load(open(foldername + 'X_train_' + str(i) + '.pickle', 'rb'))\n",
    "        X_test = pkl.load(open(foldername + 'X_test_' + str(i) + '.pickle', 'rb'))\n",
    "\n",
    "        ids_train = pkl.load(open('../preops_cv/arrest/' + 'train_ids_' + str(i) + '.pickle', 'rb'))\n",
    "        ids_test = pkl.load(open('../preops_cv/arrest/' + 'test_ids_' + str(i) + '.pickle', 'rb'))\n",
    "        train_outcome2 = inputs.outcomes[(inputs.outcomes.orlogid.isin(ids_train))].copy()\n",
    "        train_outcome2 = train_outcome2[['cardiac', 'AF', 'arrest', 'DVT_PE', 'post_aki_status', 'total_blood']].to_numpy()\n",
    "        train_outcome2 = np.where(train_outcome2> 0, 1, 0)\n",
    "        test_outcome2 = inputs.outcomes[(inputs.outcomes.orlogid.isin(ids_test))].copy()\n",
    "        test_outcome2 = test_outcome2[['cardiac', 'AF', 'arrest', 'DVT_PE', 'post_aki_status', 'total_blood']].to_numpy()\n",
    "        test_outcome2 = np.where(test_outcome2> 0, 1, 0)\n",
    "        y_train = train_outcome2[:, idx]\n",
    "        y_test = test_outcome2[:, idx]\n",
    "\n",
    "        print(outcome + ' ' + \"fold \" + str(i))\n",
    "        vae_model = pkl.load(open('./distangle_vae_model/model_fold' + str(i) + '.pickle', 'rb'))                        \n",
    "        X_test = np.delete(X_test, 163, axis=1)\n",
    "        \n",
    "        X_test = X_test.astype(np.float32)\n",
    "        # log transform\n",
    "\n",
    "        X_test = np.log(X_test + 1)\n",
    "\n",
    "        x_test = torch.from_numpy(X_test).to(\"cuda\")\n",
    "        # as float32\n",
    "        x_test = x_test.float()\n",
    "        test_dataset = torch.utils.data.TensorDataset(x_test)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "        y_pred = []\n",
    "        for batch_idx, (x_test) in enumerate(test_loader):\n",
    "            x_test = x_test[0]\n",
    "            x_recon, mu, logvar, z, classifications = vae_model(x_test)\n",
    "            # apply sigmoid on classifications\n",
    "\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            classification = classifications[:, idx, :]\n",
    "            #softmax, take first 2 logits\n",
    "            classification = nn.Softmax(dim=1)(classification[:, :2])\n",
    "\n",
    "            y_pred_ = np.argmax(classification.cpu().detach().numpy(), axis=1)\n",
    "            y_pred.append(y_pred_)\n",
    "        y_pred = np.concatenate(y_pred, axis=0)\n",
    "        targets = torch.tensor(y_test).to(\"cuda\")\n",
    "\n",
    "        \n",
    "        #load attributions\n",
    "        atts_ = pkl.load(open('./interpret' + '/attributions_fold_'+ outcome + '/' + str(i) + '.pickle', 'rb'))\n",
    "        atts_ = atts_[(y_test == y_pred)]\n",
    "        atts.append(atts_)\n",
    "    atts = np.concatenate(atts, axis=0)\n",
    "    #get atts such that the target is 1\n",
    "\n",
    "    #get feature importance\n",
    "    atts = np.mean(np.abs(atts), axis=0)\n",
    "    #get first 163 features\n",
    "    atts = atts[:163]\n",
    "    #get feature names\n",
    "    cols = cols[:163]\n",
    "    #sort features and get indices from most important to least\n",
    "    #get probabilities of each feature\n",
    "    atts = atts/np.sum(atts)\n",
    "    sorted_indices = np.argsort(atts)\n",
    "    #get top features from most important to least\n",
    "    top_features = [cols[i] for i in sorted_indices[-10:]]\n",
    "\n",
    "    print(top_features)\n",
    "    #Plot top features\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.barh(top_features, atts[sorted_indices[-10:]])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Preoperative Features')\n",
    "    plt.title('Top 10 Features for ' + outcome )\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import combinations\n",
    "from numbers import Number\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import torch\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import *\n",
    "import argparse\n",
    "from transform import *\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "with open('../../../inputs_new.pickle','rb') as handle:\n",
    "    inputs = pkl.load(handle)\n",
    "print(inputs)\n",
    "cols = inputs.preops.columns\n",
    "#get rid of first and last\n",
    "cols = cols[1:-1]    \n",
    "#change column name orlogid_encoded to orlogid\n",
    "inputs.outcomes.rename(columns={'orlogid_encoded':'orlogid'}, inplace=True)\n",
    "inputs.texts.rename(columns={'orlogid_encoded':'orlogid'}, inplace=True)\n",
    "print(inputs.outcomes.columns)\n",
    "folder = ('../preops_cv/')\n",
    "\n",
    "outcomes = ['cardiac', 'AF','arrest', 'DVT_PE', 'post_aki_status', 'total_blood']\n",
    "for idx, outcome in enumerate(outcomes):\n",
    "    foldername = folder+'arrest'+'/'\n",
    "    atts = []\n",
    "    for i in range(5):\n",
    "        X_train = pkl.load(open(foldername + 'X_train_' + str(i) + '.pickle', 'rb'))\n",
    "        X_test = pkl.load(open(foldername + 'X_test_' + str(i) + '.pickle', 'rb'))\n",
    "\n",
    "        ids_train = pkl.load(open('../preops_cv/arrest/' + 'train_ids_' + str(i) + '.pickle', 'rb'))\n",
    "        ids_test = pkl.load(open('../preops_cv/arrest/' + 'test_ids_' + str(i) + '.pickle', 'rb'))\n",
    "        train_outcome2 = inputs.outcomes[(inputs.outcomes.orlogid.isin(ids_train))].copy()\n",
    "        train_outcome2 = train_outcome2[['cardiac', 'AF', 'arrest', 'DVT_PE', 'post_aki_status', 'total_blood']].to_numpy()\n",
    "        train_outcome2 = np.where(train_outcome2> 0, 1, 0)\n",
    "        test_outcome2 = inputs.outcomes[(inputs.outcomes.orlogid.isin(ids_test))].copy()\n",
    "        test_outcome2 = test_outcome2[['cardiac', 'AF', 'arrest', 'DVT_PE', 'post_aki_status', 'total_blood']].to_numpy()\n",
    "        test_outcome2 = np.where(test_outcome2> 0, 1, 0)\n",
    "        y_train = train_outcome2[:, idx]\n",
    "        y_test = test_outcome2[:, idx]\n",
    "\n",
    "        print(outcome + ' ' + \"fold \" + str(i))\n",
    "        vae_model = pkl.load(open('./distangle_vae_model_epoch/model_fold' + str(i) + '.pickle', 'rb'))                        \n",
    "        X_test = np.delete(X_test, 163, axis=1)\n",
    "        \n",
    "        X_test = X_test.astype(np.float32)\n",
    "        # log transform\n",
    "\n",
    "        X_test = np.log(X_test + 1)\n",
    "\n",
    "        x_test = torch.from_numpy(X_test).to(\"cuda\")\n",
    "        # as float32\n",
    "        x_test = x_test.float()\n",
    "        test_dataset = torch.utils.data.TensorDataset(x_test)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "        y_pred = []\n",
    "        for batch_idx, (x_test) in enumerate(test_loader):\n",
    "            x_test = x_test[0]\n",
    "            x_recon, mu, logvar, z, classifications = vae_model(x_test)\n",
    "            # apply sigmoid on classifications\n",
    "\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            classification = classifications[:, idx, :]\n",
    "            #softmax, take first 2 logits\n",
    "            classification = nn.Softmax(dim=1)(classification[:, :2])\n",
    "\n",
    "            y_pred_ = np.argmax(classification.cpu().detach().numpy(), axis=1)\n",
    "            y_pred.append(y_pred_)\n",
    "        y_pred = np.concatenate(y_pred, axis=0)\n",
    "        targets = torch.tensor(y_test).to(\"cuda\")\n",
    "\n",
    "        \n",
    "        #load attributions\n",
    "        atts_ = pkl.load(open('./interpret' + '/attributions_fold_'+ outcome + '/' + str(i) + '.pickle', 'rb'))\n",
    "        atts_ = atts_[(y_test == y_pred)]\n",
    "        atts.append(atts_)\n",
    "    atts = np.concatenate(atts, axis=0)\n",
    "    #get atts such that the target is 1\n",
    "\n",
    "    #get feature importance\n",
    "    atts = np.mean(np.abs(atts), axis=0)\n",
    "    #get first 163 features\n",
    "    atts = atts[:163]\n",
    "    #get feature names\n",
    "    cols = cols[:163]\n",
    "    #sort features and get indices from most important to least\n",
    "    #get probabilities of each feature\n",
    "    atts = atts/np.sum(atts)\n",
    "    sorted_indices = np.argsort(atts)\n",
    "    #get top features from most important to least\n",
    "    top_features = [cols[i] for i in sorted_indices[-12:]]\n",
    "\n",
    "    print(top_features)\n",
    "    #store into csv\n",
    "    df = DataFrame({'Feature': top_features, 'Importance': atts[sorted_indices[-12:]]})\n",
    "    df.to_csv('./interpret' + '/top_features_fold_'+ outcome + '.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import combinations\n",
    "from numbers import Number\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import torch\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import *\n",
    "import argparse\n",
    "from transform import *\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "with open('../../../inputs_new.pickle','rb') as handle:\n",
    "    inputs = pkl.load(handle)\n",
    "print(inputs)\n",
    "cols = inputs.preops.columns\n",
    "#get rid of first and last\n",
    "cols = cols[1:-1]    \n",
    "#change column name orlogid_encoded to orlogid\n",
    "inputs.outcomes.rename(columns={'orlogid_encoded':'orlogid'}, inplace=True)\n",
    "inputs.texts.rename(columns={'orlogid_encoded':'orlogid'}, inplace=True)\n",
    "print(inputs.outcomes.columns)\n",
    "folder = ('../preops_cv/')\n",
    "\n",
    "outcomes = ['cardiac', 'AF','arrest', 'DVT_PE', 'post_aki_status', 'total_blood']\n",
    "for idx, outcome in enumerate(outcomes):\n",
    "    foldername = folder+'arrest'+'/'\n",
    "    aurocs = []\n",
    "    auprcs = []\n",
    "    i = 1\n",
    "\n",
    "    X_train = pkl.load(open(foldername + 'X_train_' + str(i) + '.pickle', 'rb'))\n",
    "    X_test = pkl.load(open(foldername + 'X_test_' + str(i) + '.pickle', 'rb'))\n",
    "\n",
    "    ids_train = pkl.load(open('../preops_cv/arrest/' + 'train_ids_' + str(i) + '.pickle', 'rb'))\n",
    "    ids_test = pkl.load(open('../preops_cv/arrest/' + 'test_ids_' + str(i) + '.pickle', 'rb'))\n",
    "    train_outcome2 = inputs.outcomes[(inputs.outcomes.orlogid.isin(ids_train))].copy()\n",
    "    train_outcome2 = train_outcome2[['cardiac', 'AF', 'arrest', 'DVT_PE', 'post_aki_status', 'total_blood']].to_numpy()\n",
    "    train_outcome2 = np.where(train_outcome2> 0, 1, 0)\n",
    "    test_outcome2 = inputs.outcomes[(inputs.outcomes.orlogid.isin(ids_test))].copy()\n",
    "    test_outcome2 = test_outcome2[['cardiac', 'AF', 'arrest', 'DVT_PE', 'post_aki_status', 'total_blood']].to_numpy()\n",
    "    test_outcome2 = np.where(test_outcome2> 0, 1, 0)\n",
    "    y_train = train_outcome2[:, idx]\n",
    "    y_test = test_outcome2[:, idx]\n",
    "\n",
    "    print(outcome + ' ' + \"fold \" + str(i))\n",
    "\n",
    "    vae_model = pkl.load(open('./distangle_vae_model/model_fold' + str(i) + '.pickle', 'rb'))\n",
    "\n",
    "    X_test = np.delete(X_test, 163, axis=1)\n",
    "    i_list = [549] #select a random patient\n",
    "\n",
    "    X_test = X_test[i_list]\n",
    "    y_test = y_test[i_list]\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    # log transform\n",
    "\n",
    "    X_test = np.log(X_test + 1)\n",
    "\n",
    "    vae_model = vae_model.to(\"cuda\")\n",
    "\n",
    "    #construct model using encoder of vae and classifier\n",
    "    class VAE(nn.Module):\n",
    "        def __init__(self, vae_model):\n",
    "            super(VAE, self).__init__()\n",
    "            self.feature_encoder = vae_model.feature_encoder\n",
    "            self.encoder = vae_model.encoder\n",
    "            self.classifier = vae_model.classifiers[idx]\n",
    "        def forward(self, x):\n",
    "            x = self.feature_encoder(x)\n",
    "            x = self.encoder(x)\n",
    "            x = self.classifier(x)\n",
    "            #softmax\n",
    "            x = nn.Softmax(dim=1)(x[:, :2])\n",
    "            return x\n",
    "    model = VAE(vae_model)\n",
    "    model = model.to(\"cuda\")\n",
    "    model.train()\n",
    "\n",
    "    ig = IntegratedGradients(model)\n",
    "    x_test = torch.from_numpy(X_test).to(\"cuda\")\n",
    "    # as float32\n",
    "    x_test = x_test.float()\n",
    "\n",
    "    targets = torch.tensor(y_test).to(\"cuda\")\n",
    "    #use batch size of 64\n",
    "    dataset = torch.utils.data.TensorDataset(x_test, targets)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "    #use integrated gradients\n",
    "    atts = []\n",
    "    for batch in dataloader:\n",
    "        x, y = batch\n",
    "        x.requires_grad = True\n",
    "        attributions = ig.attribute(x, target=y, n_steps=100)\n",
    "        atts.append(attributions.cpu().detach().numpy())\n",
    "        #release memory\n",
    "        del x\n",
    "        del y\n",
    "        del attributions\n",
    "\n",
    "    #release memory\n",
    "    del x_test\n",
    "    del targets\n",
    "    del dataset\n",
    "    del dataloader\n",
    "\n",
    "    atts = np.concatenate(atts, axis=0)\n",
    "\n",
    "    #save attributions\n",
    "    if not os.path.exists('./interpret_single'):\n",
    "        os.makedirs('./interpret_single')\n",
    "    if not os.path.exists('./interpret_single' + '/attributions_fold_'+ outcome):\n",
    "        os.makedirs('./interpret_single' + '/attributions_fold_'+ outcome)\n",
    "    pkl.dump(atts, open('./interpret_single' + '/attributions_fold_'+ outcome + '/' + str(i) + '.pickle', 'wb'))\n",
    "    atts = np.mean(np.abs(atts), axis=0)\n",
    "    #get first 163 features\n",
    "    atts = atts[:163]\n",
    "    #get feature names\n",
    "    cols = cols[:163]\n",
    "    #sort features and get indices from most important to least\n",
    "    #get probabilities of each feature\n",
    "    atts = atts/np.sum(atts)\n",
    "    sorted_indices = np.argsort(atts)\n",
    "    #get top features from most important to least\n",
    "    top_features = [cols[i] for i in sorted_indices[-12:]]\n",
    "    print(top_features)\n",
    "\n",
    "    #store into csv\n",
    "    df = DataFrame({'Feature': top_features, 'Importance': atts[sorted_indices[-12:]]})\n",
    "    df.to_csv('./interpret_single' + '/top_features_fold_'+ outcome + '.csv', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
